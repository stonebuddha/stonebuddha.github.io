[{"authors":null,"categories":null,"content":"我是北京大学计算机学院的一名助理教授。 我在卡内基梅隆大学计算机系获得博士学位，导师为 Jan Hoffmann 教授。 我目前的研究方向为编程语言、量化验证、概率编程，我的研究兴趣也包括类型理论、程序合成、并发编程、贝叶斯推断。\n我在北京大学计算机科学与技术系获得学士学位，导师为熊英飞教授。 我的本科生毕业设计研究了基于 Datalog 条件摘要的程序分析加速技术。\n这是我的简历。\n新闻 欢迎对编程语言领域感兴趣的学生联系我！我在这里概述了我的一些研究想法。 [Sep 2022] Probabilistic Resource-Aware Session Types 被 POPL 2023 接收。 [May 2022] 我完成了博士学位答辩。我的博士论文为 Static Analysis of Probabilistic Programs: An Algebraic Approach。 [Sep 2021] 我开始找 2022 年入职的学术界工作！ [Feb 2021] 两篇论文 Sound Probabilistic Inference via Guide Types 和 Central Moment Analysis for Cost Accumulators in Probabilistic Programs 被 PLDI 2021 接收。 老新闻归档至此处。\n","date":1667123397,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1667123397,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"我是北京大学计算机学院的一名助理教授。 我在卡内基梅隆大学计算","tags":null,"title":"王迪","type":"authors"},{"authors":["王迪"],"categories":[],"content":"人们近年来在人工智能领域取得的进展，除了层出不穷的新算法，也仰仗硬件算力的提升并得益于编程语言层面对自动微分和张量运算的支持。 基于神经网络的机器学习在很多应用中取得了成功，但在一些方面也尚存不足，例如对预测结果信度的评估1，纳入领域知识的方法2，以及在观测数据的分布发生改变时的鲁棒性3。 贝叶斯推断（Bayesian Inference）是一种历史悠久的、基于概率的贝叶斯解释（而非频率解释）的、在上述三个方面有一些优势的机器学习方法，其核心在于通过观测数据来估计模型假设空间的概率分布，而非选择出单个“最优”假设：设 $\\Theta$ 为假设空间、$x$ 为观测数据，每个假设 $\\theta \\in \\Theta$ 具有一个先验概率 $\\mathbb{P}(\\theta)$，而一个贝叶斯模型则描述了观测数据在给定假设下的条件概率 $\\mathbb{P}(x \\mid \\theta)$，那么由贝叶斯法则，我们可用观测数据更新模型假设的后验概率： $$ \\mathbb{P}(\\theta \\mid x) = \\frac{ \\mathbb{P}(x \\mid \\theta) \\mathbb{P}(\\theta) }{ \\sum_{\\theta’ \\in \\Theta} \\mathbb{P}(x \\mid \\theta’) \\mathbb{P}(\\theta’) } \\qquad (\\theta \\in \\Theta). $$ 基于贝叶斯推断的机器学习也被称为贝叶斯机器学习4 5。 然而，相比基于梯度下降等算法的机器学习方法，贝叶斯推断的计算复杂度更高，这阻挡了贝叶斯机器学习在更多领域、更大模型上的应用。\n为了更快更好地进行贝叶斯推断，概率编程6逐渐成为一个活跃的研究领域。 概率编程旨在分离描述概率模型和进行贝叶斯推断这两个步骤，通过良好的编程语言设计来支持丰富的模型种类，通过编程语言的各类技术（如编译优化、动态分析、元编程等）来自动、高效地进行贝叶斯推断。 人们已经设计、开发了多个概率编程语言（如 Stan、Pyro、Gen.jl 等），但如何平衡正确性和灵活性仍然是该领域的一个重要研究问题。 一方面，设计有语义限制的概率编程语言并提供特化的推断算法，可以保证正确性和高效性，但是语言可表达的模型种类受限，也难以重用已有的非概率的计算模块；另一方面，使用通用编程语言来描述概率模型，并允许用户对通用推断算法进行定制，可以满足灵活性和一定程度上的高效性7 8，但是用户定制会使得正确性的保证变得困难。 正如自动微分框架驱动了基于神经网络的机器学习的发展，我们期待一个正确而灵活的概率编程框架能驱动贝叶斯机器学习的进一步发展和流行。\n在这里，我们看一个使用概率编程来进行时间序列的在线学习的例子9。 该例子希望学习并预测美国的月度民航总里程数的走势（数据来源）。下图为 2009 年 1 月至 2020 年 2 月的数据： 在贝叶斯机器学习中，高斯过程回归（Gaussian Process Regression）是一种灵活的算法，该方法允许我们使用核函数（Kernel Function）来定制多元高斯分布的的协方差矩阵。 通过概率编程，我们可以不用预先指定使用什么形式的核函数，而是写一段程序来声明核函数形式的先验概率分布（换句话说，我们可以把结构的选择也纳入模型假设 $\\theta$ 中）。 下面的代码通过概率上下文无关文法（Probabilistic Context-Free Grammar）的方式实现了这样的一个先验概率分布：\ntype kernel = | Constant of float | Linear of float | ... | Plus of kernel * kernel | ... let rec kernel_prior () = let kernel_type = categorical [0.2; 0.2; 0.2; 0.2; 0.1; 0.1] in match kernel_type with | 0 -\u0026gt; Constant (rand ()) (* Constant (C): k(x, x\u0026#39;) = C *) | 1 -\u0026gt; Linear (rand ()) (* Linear (C): k(x, x\u0026#39;) = (x - C) * (x\u0026#39; - C) *) ... | 4 -\u0026gt; Plus (kernel_prior (), kernel_prior ()) (* Plus (k1, k2): k(x, x\u0026#39;) = k1(x, x\u0026#39;) + k2(x, x\u0026#39;) *) ... 结合序列蒙特卡洛（Sequential Monte Carlo）方法，我们可以实现一个基于高斯过程的时间序列的在线学习算法，该算法可以估计预测的不确定性，并在数据分布发生改变时即时响应。 下面这个动画展示了在线学习的效果，其中灰色的区域为使用 100 个对后验概率分布的采样做出的 95% 置信度预测区间的叠加： 可以看出，在数据比较多的时候，算法已经对整个趋势的掌握已经非常不错了。而下面的动画则展示在 2020 年 2 月之后的数据上的效果： 很明显，新冠疫情导致了民航数据的突变，而基于贝叶斯推断的在线学习可以及时对这种变化进行响应并调整之后的预测。\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. 2017. On Calibration of Modern Neural Networks. ICML\u0026#39;17. ↩︎\nNikhil Muralidhar, Mohammad Raihanul Islam, Manish Marwah, Anuj Karpatne, and Naren Ramakrishnan. 2018. Incorporating Prior Domain Knowledge into Deep Neural Networks. ICBD\u0026#39;18. ↩︎\nDario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman,and Dan Mané. 2016. Concrete Problems in AI Safety. https://arxiv.org/abs/1606.06565 ↩︎\nZoubin Ghahramani. 2015. Probabilistic machine learning and artiicial intelligence. Nature, 521. ↩︎\nJoshua B. Tenenbaum, Charles Kemp, Thomas L. Grifiths, and Noah D. Goodman. 2011. How to Grow a Mind: Statistics, Structure, and Abstraction. Science, 331, 6022. ↩︎\nJan Willem van de Meent, Brooks Paige, Hongseok Yang, and Frank Wood. 2018. An Introduction to Probabilistic Programming. https://arxiv.org/abs/1809.10756 ↩︎\nVikash K. Mansinghka, Ulrich Schaechtle, Shivam Handa, Alexey Radul, Yutian Chen, and Martin C. Rinard. 2018. Probabilistic Programming with Programmable Inference. PLDI\u0026#39;18. ↩︎\nEli Bingham, Jonathan P. Chen, Martin Jankowiak, Fritz Obermeyer, Neeraj Pradhan, Theofanis Karaletsos, Rishabh Singh, Paul Szerlip, Paul Horsfall, and Noah D. Goodman. 2018. Pyro: Deep Universal Probabilistic Programming. J. Machine Learning Research, 20, 1. ↩︎\nFeras A. Saad, Marco F. Cusumano-Towner, Ulrich Schaechtle, Martin C. Rinard, and Vikash K. Mansinghka. 2019. Bayesian Synthesis of Probabilistic Programs for Automatic Data. POPL\u0026#39;19. ↩︎\n","date":1667123397,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1667123397,"objectID":"7d130f80fb0a0c4948b3e6fb635e0362","permalink":"https://stonebuddha.github.io/zh/post/introduction-to-probabilistic-programming/","publishdate":"2022-10-30T17:49:57+08:00","relpermalink":"/zh/post/introduction-to-probabilistic-programming/","section":"post","summary":"人们近年来在人工智能领域取得的进展，除了层出不穷的新算法，也","tags":[],"title":"随便聊聊：贝叶斯概率编程","type":"post"},{"authors":["王迪"],"categories":[],"content":"本文参考了 Zachary Kincaid 和 Thomas Reps 在 CAV 2021 上的关于代数程序分析（Algebraic Program Analysis）的教程1 2。\n代数程序分析简单来说就是一种基于代数结构来设计程序分析的方法论：某种代数结构中的元素代表了程序的含义，而其上的运算则代表组合程序含义的方式。 这其实跟我们在程序语法树上用结构递归定义指称语义（Denotational Semantics）的方式类似： $$ \\begin{align} \\mathcal{A}[{-}] \u0026amp; : \\mathit{Program} \\to \\mathit{Meaning} \\\\ \\mathcal{A}[S_1;S_2] \u0026amp; = \\mathcal{A}[S_1] \\cdot \\mathcal{A}[S_2] \\\\ \\mathcal{A}[\\mathbf{if}(*)\\{S_1\\}\\mathbf{else}\\{S_2\\}] \u0026amp; = \\mathcal{A}[S_1] + \\mathcal{A}[S_2] \\\\ \\mathcal{A}[\\mathbf{while}(*)\\{S_0\\}] \u0026amp; = (\\mathcal{A}[S_0])^* \\end{align} $$ 而上面式子中的 $\\cdot$、$+$ 和 $^*$ 就可以视作某种代数结构所支持的运算。 这种方式具有良好的 Compositionality：一个程序的含义总是由该程序的组成部分的含义结合而来。 例如，如果上面的 $\\mathcal{A}$ 代表了一个程序分析，那么我们需要指定如何实现连接（$\\cdot$）、分支（$+$）和循环（$^*$）这三种运算。 注意，这里出现了一个与传统基于迭代的程序分析的重要不同：一个循环语句的含义并不是由反复迭代循环体直到不动点来获得，而是通过一个显式的 $^*$ 运算来获得。 换句话说，代数程序分析允许我们使用别的（非迭代的）方式来分析循环，这就为我们设计新的程序分析技术提供了可能性。\n在本文中，我们考虑通过状态转移公式代数（Transition Formula Alegebras）来分析程序变量间的数值关系。 一个状态转移公式$F(X,X’)$是一个逻辑公式，它描述了程序状态上的转移关系：$X$ 集合表示前状态（pre-state）的变量，$X’$ 集合表示后状态（post-state）的变量。 比如我们考虑变量有 $x,y$，那么程序语句 x = x + 1; 的状态转移公式就是 $x’ = x + 1 \\wedge y’ = y$。 对于一个固定的变量集合 $X$，我们考虑在所有可能的状态转移公式 $F(X,X’)$ 上建立一个适用于程序分析的代数结构。 在传统程序分析中，我们往往需要预先对可能的公式进行限制来使得迭代算法可以收敛：例如，只考虑变量间的线性不等式。 在代数程序分析中，我们不需要这种预先的限制，而是可以在实现循环运算（$^*$）时以即插即用的方式使用不同的近似方法。 在状态转移公式代数中，我们定义常数 $0$ 为 $\\mathit{false}$，常数 $1$ 为 $\\bigwedge_{x \\in X} (x’ = x)$，分支运算为 $F + G = F \\vee G$，连接运算为 $F \\cdot G = \\exists X’’. F(X,X’’) \\wedge G(X’’,X’)$（即用 $X’’$ 来表示中间状态）。 而对于循环运算 $({-})^* : \\mathit{TransitionFormula} \\to \\mathit{TransitionFormula}$，我们只需考虑在转移公式这一层面进行计算，并不用考虑循环语句本身可能有的嵌套循环结构！ 换句话说，我们可以借鉴已有的各种 Loop Summarization / Acceleration 技术。\n在这里，我们只讨论一种基于区间分析的循环运算实现方法（本文开头提到的参考材料中有更多的例子）。 在区间分析中，我们一般考虑形如 $\\bigwedge_{x \\in X} (a_x \\le x \\le b_x)$ 的状态公式，其中 $a_x,b_x$ 为常数。 例如，对于下面这个程序中的循环而言，$0 \\le i \\le 10 \\wedge 0 \\le j \\le 20$ 是一个区间不变量，但是 $0 \\le i \\le 10 \\wedge 0 \\le j \\le 10$ 并不是：\ni = 0; j = 0; while (i \u0026lt; 10 \u0026amp;\u0026amp; j != 20 \u0026amp;\u0026amp; j \u0026lt; 100) { i = i + 1; j = j + 1; } 传统的基于区间的程序分析会使用 widening / narrowing 等技术来确保迭代分析可以收敛，但这会影响分析的精度。 在代数程序分析的框架中，我们则拥有更好的自由度来设计对于循环的分析：考虑循环体的转移公式为 $F(X,X’)$，那么断言“$A = \\{ a_x \\mid x \\in X \\}$ 和 $B = \\{ b_x \\mid x \\in X \\}$构成一个区间不变量”可以表述为下面的公式： $$ \\forall X, X’. \\left(\\left(\\bigwedge_{x \\in X}(a_x \\le x \\le b_x)\\right) \\wedge F(X,X’)\\right) \\implies \\bigwedge_{x \\in X} (a_x \\le x’ \\le b_x) $$ 令 $Inv(A,B)$ 为上面这个式子。那么循环运算 $F^{*}$ 可以定义为： $$ \\forall A, B. \\left( Inv(A,B) \\wedge \\bigwedge_{x \\in X} (a_x \\le x \\le b_x) \\right) \\implies \\bigwedge_{x \\in X} (a_x \\le x’ \\le b_x) $$ 这种循环运算的实现蕴含了所有可以由循环体转移公式 $F$ 得出的区间不变量！\n前面我们提到，代数程序分析要求在程序语法树上进行结构递归，那么在非结构的程序上（例如有 break 和 continue），我们是否还能进行代数程序分析呢？ 答案再次由 Robert Tarjan 给出：Yes！（至于为什么用“再”可以看我上次的分享。) Tarjan 的两篇文章 Fast Algorithms for Solving Path Problems 和 A Unified Approach to Path Problems 中提出了一种高效的基于代数的在图上解决路径问题的算法：这里的路径问题指的是图中的边上有权值，一条路径的权值为其中边权相乘，然后我们想要计算符合某种条件的所有路径的权值和（这里的乘和和都是抽象的，例如如果权值为非负数，乘为加，和为取最小值，那么路径问题描述的则是最短路问题）。 Tarjan 提出了一种可以计算图上两点间所有路径的集合的高效算法，其关键点在于这个（可能无穷的）路径集合可以表述为一个有限的正则表达式，其后解路径问题就可以转化为在一个描述问题的代数结构中解释正则表达式：这与我们上文描述的三种运算（连接 $\\cdot$，分支 $+$，循环 $^*$）正好是符合的！ 所以，对于非结构的程序，我们可以把它表示成一个控制流图，然后通过 Tarjan 的算法计算出描述所有可能的程序执行路径的正则表达式，最后在描述程序分析的代数结构中解释该正则表达式。 例如，以下程序\nwhile (true) { m = 0; while (m \u0026lt; 8) { if (n \u0026lt; 0) { return; } else { m = m + 1; n = n - 1; } } } 中的所有可能路径的集合可以表述为下面的正则表达式： $$\\small \\left(\\fbox{m=0} \\cdot \\left(\\fbox{m\u0026lt;8} \\cdot \\fbox{n\u0026gt;=0} \\cdot \\fbox{m=m+1} \\cdot \\fbox{n=n-1}\\right)^* \\cdot \\fbox{m\u0026gt;=8}\\right)^* \\cdot \\fbox{m=0} \\cdot \\fbox{m\u0026lt;8} \\cdot \\fbox{n\u0026lt;0} $$\n在文章的最后，我们聊一聊代数程序分析的不足。 最明显的不足在于，尽管 Compositionality 是个好性质，它也意味着我们在程序分析中丢失了上下文信息。 一方面，由于在分析某个程序组成部分时不能依赖其上下文，我们在分析时需要追踪更多的信息（例如上文的状态转移公式代数需要同时记录 $X$ 和 $X’$）；另一方面，我们可能会由于缺乏上下文信息而对某个程序组成部分进行过于保守的分析（这取决于程序分析状态空间设计得好不好）。\nZachary Kincaid, Thomas Reps, and John Cyphert. Algebraic Program Analysis. https://doi.org/10.1007/978-3-030-81685-8_3 ↩︎\nZachary Kincaid and Thomas Reps. Introduction to Algebraic Program Analysis. Part 1, Part 2, Part 3, Part 4 ↩︎\n","date":1664722271,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1664722271,"objectID":"77b7cdc4ebaeb305431f0d0b85ae5883","permalink":"https://stonebuddha.github.io/zh/post/introduction-to-algebraic-program-analysis/","publishdate":"2022-10-02T22:51:11+08:00","relpermalink":"/zh/post/introduction-to-algebraic-program-analysis/","section":"post","summary":"本文参考了 Zachary Kincaid 和 Thomas Reps 在 CAV 2021 上的关于代数程序分析（Algebr","tags":[],"title":"随便聊聊：代数程序分析","type":"post"},{"authors":["王迪"],"categories":[],"content":"简单地说，程序的资源分析（Resource Analysis）指的是把该程序的资源消耗表示成一个关于程序输入的函数。 这里的资源可以是运行时间、内存使用、能源消耗，或别的什么数值指标。 高德纳（Donald Knuth）在《计算机程序设计的艺术》（The Art of Computer Programming）中就基于 MIX 汇编语言的语义讨论了一些资源分析的问题。 在计算机科学的课程中，资源分析通常以分析算法、数据结构的时间、空间复杂度的形式出现，而这里面最常见的又通常是渐近分析，即忽略常数、假定输入规模充分大时的复杂度，例如对长度为 $n$ 的数组进行归并排序时间复杂度为 $O(n \\log n)$。 在这篇文章中，我们考虑一个稍微困难点的情况，即我们希望得到的分析结果是带有常数信息的非渐近复杂度：从非渐近分析可以容易得出渐近分析的结果，而且前者还可以用来精细地比较渐近复杂度相同的算法、数据结构。\n静态资源分析即在不实际运行程序的情况下对其进行资源分析。 Facebook 出品的 Infer 工具提供了函数级的静态运行时间分析，其宗旨是在软件开发的过程中更早地指出性能问题（比方说，在 Code Review 阶段由工具自动反馈，而不是等到后面的性能回归测试）。 下面这个例子来源于 Infer 的官方文档1：\nvoid loop(ArrayList\u0026lt;Integer\u0026gt; list){ for (int i = 0; i \u0026lt;= list.size(); i++){ } } Infer 通过静态分析对上面这个函数得出一个描述其运行时间的多项式（比如 $8|list|+16$），从而知道这个函数的时间复杂度和其输入列表的长度呈线性关系。 如果某一次代码修改把这个函数更新为：\nvoid loop(ArrayList\u0026lt;Integer\u0026gt; list){ for (int i = 0; i \u0026lt;= list.size(); i++){ foo(i); // newly added function call } } 而此处的 foo 函数的时间复杂度也是与输入呈线性关系，那么 Infer 可以检测到 loop 函数的复杂度从 $O(|list|)$ 增长到了 $O(|list|^2)$，并向开发者发出警报。 （如果想要更深入了解 Infer 中的资源分析，可以参考这个视频。）\n笼统地说，Infer 中的资源分析可以大致理解为对循环次数进行计数，从而将资源分析问题转化为程序变量数值关系的分析；这也是很多静态资源分析（比如 SPEED2）背后的原理。 在这篇文章中，我们来聊另一种颇为有趣的静态资源分析：自动均摊资源分析（Automatic Amortized Resource Analysis，AARA）。 这种方法衍生自 Robert Tarjan 在 1985 年的一篇论文3，文中给出了一种推导序列操作的最坏情况资源消耗的方法。 该方法的思路可以概括为：对于很多数据结构来说，一个操作消耗的资源很大程度上是由数据结构的状态决定的，而且有可能会根据状态有很大的不同；但是比较高的资源消耗（例如重新组织整个数据结构）往往会以一种可以预测的频率出现，也就是说，这些消耗在时间轴上均摊了。 均摊资源分析即利用了这种思想：我们可以把程序的执行看做是一个操作序列 $s_1,s_2,s_3,\\cdots$，这里的每个 $s_i \\in \\mathrm{State}$ 都表示一个程序状态，而我们用一个函数 $cost(s_i,s_{i+1})$ 来描述资源的消耗；然后我们设计一个势能函数 $\\Phi : \\mathrm{State} \\to \\mathbb{Q}^+$ 把程序状态映射为非负数，使得对任意的 $i$，我们有 $$ \\Phi(s_{i}) \\ge cost(s_{i}, s_{i+1}) + \\Phi(s_{i+1}), $$ 即一个状态的势能要足以支付当前程序操作的资源消耗以及下一个程序状态的势能（思考题：$cost$ 为负时意味着什么？）。 如此以来，$\\Phi(s_0)$ 给出了程序的总资源消耗的一个上界，而我们需要做的是定义一个势能函数，并证明在每个程序执行的局部，上述表达式成立。 这意味着均摊资源分析的 Compositionality 是很好的！\n我们知道，类型系统的一大特点也是 Compositionality；所以，设计一个用于均摊资源分析的类型系统是非常自然的想法。 考虑下面这段 OCaml 代码，它是连接两个列表的简单实现，而我们想要分析它递归调用的次数（这也是一种资源消耗）：\nlet rec append (l1, l2) = match l1 with | [] -\u0026gt; l2 | x :: xs -\u0026gt; let rest = append (xs, l2) in x :: rest 我们能够很容易地看出 append 函数递归调用的次数是 $|l_1|$。 假定这个 append 函数的类型为 $L(\\alpha) \\times L(\\alpha) \\to L(\\alpha)$，这里的 $L$ 即表示列表类型。 均摊分析要求我们给程序状态赋予势能，在这个例子中，我们则需要把我们的势能放在 append 的参数 $l_1$ 和 $l_2$ 里。 让我们给列表类型增加一个数值标注：用 $L^q(\\alpha)$ 表示一个列表类型，其中的每个元素都携带了 $q$ 单位的势能，并假定所有程序操作的资源消耗都是 $0$，除了递归调用的消耗为 $1$ 单位。 所以，append 的带资源标注的类型可以写为 $L^1(\\alpha) \\times L^0(\\alpha) \\to L^0(\\alpha)$：如果我们计算参数和结果之间的势能差，我们就能发现那恰好是 $|l_1|$。 但是，我们如何通过类型系统来检查这个类型是不是正确的呢？ 还记得 Compositionality 吗——我们只需要对局部操作验证一下势能不等式：比如，在模式匹配时，如果 $l_1$ 非空，那么我们把它解构为 $x$ 和 $xs$，其中 $xs$ 的类型与 $l_1$ 相同，为 $L^1(\\alpha)$，但是 $xs$ 的长度比 $l_1$ 少 $1$，所以这一步解构给了我们 $1$ 单位的自由势能，我们可以用它来支付紧接着的这个递归调用。 接着我们检查 $xs$ 和 $l_2$ 的类型是否符合 append 的类型签名（当然！）。 最后，类型签名告诉我们 $rest$ 的类型为 $L^0(\\alpha)$，所以构建最终返回结果 $x :: rest$ 也不需要往这个数据结构里面存势能了。\n我在这份胶片里更详细地展示了这个带资源标注的类型系统。 需要特别提出的是，自动推导这些数值标注并不困难：我们用变量来表示这些类型标注，那么类型检查就成为了生成关于这些变量的约束（通常是线性的）的过程，最后再解这些约束就好了。 在这个页面上你可以和一个实现了自动均摊资源分析的工具（Resource-aware ML，RaML）玩耍，其主要贡献者也在今年发表了一篇还不错的综述4。 祝大家玩得愉快！\nhttps://fbinfer.com/docs/checker-cost#examples ↩︎\nhttps://www.microsoft.com/en-us/research/publication/speed-precise-and-efficient-static-estimation-of-program-computational-complexity-2/ ↩︎\nRobert Tarjan. Amortized Computational Complexity. https://doi.org/10.1137/0606031 ↩︎\nJan Hoffmann and Steffen Jost. Two decades of automatic amortized resource analysis. https://doi.org/10.1017/S0960129521000487 ↩︎\n","date":1662795139,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1662795139,"objectID":"ecc5d483fca4ca39896af5aa4d67d9db","permalink":"https://stonebuddha.github.io/zh/post/introduction-to-static-resource-analysis/","publishdate":"2022-09-10T15:32:19+08:00","relpermalink":"/zh/post/introduction-to-static-resource-analysis/","section":"post","summary":"简单地说，程序的资源分析（Resource Analysis）","tags":[],"title":"随便聊聊：静态资源分析","type":"post"},{"authors":["王迪","Jan Hoffmann","Thomas Reps"],"categories":[],"content":"","date":1624147200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1624147200,"objectID":"45117ebf3164126815c2285cafe7bc4a","permalink":"https://stonebuddha.github.io/zh/publication/wanghr21a/","publishdate":"2021-09-15T11:30:37+08:00","relpermalink":"/zh/publication/wanghr21a/","section":"publication","summary":"For probabilistic programs, it is usually not possible to automatically derive exact information about their properties, such as the distribution of states at a given program point. Instead, one can attempt to derive approximations, such as upper bounds on *tail probabilities*. Such bounds can be obtained via concentration inequalities, which rely on the *moments* of a distribution, such as the expectation (the first *raw* moment) or the variance (the second *central* moment). Tail bounds obtained using central moments are often tighter than the ones obtained using raw moments, but automatically analyzing central moments is more challenging.\n\nThis paper presents an analysis for probabilistic programs that automatically derives symbolic upper and lower bounds on variances, as well as higher central moments, of *cost accumulators*. To overcome the challenges of higher-moment analysis, it generalizes analyses for expectations with an algebraic abstraction that simultaneously analyzes different moments, utilizing relations between them. A key innovation is the notion of *moment-polymorphic recursion*, and a practical derivation system that handles recursive functions.\n\nThe analysis has been implemented using a template-based technique that reduces the inference of polynomial bounds to linear programming. Experiments with our prototype central-moment analyzer show that, despite the analyzer's upper/lower bounds on various quantities, it obtains tighter tail bounds than an existing system that uses only raw moments, such as expectations.","tags":[],"title":"Central Moment Analysis for Cost Accumulators in Probabilistic Programs","type":"publication"},{"authors":["王迪","Jan Hoffmann","Thomas Reps"],"categories":[],"content":"","date":1624147200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1624147200,"objectID":"1c9c209b3c8ad03bd0f0d48a6fa71e7c","permalink":"https://stonebuddha.github.io/zh/publication/wanghr21b/","publishdate":"2021-09-15T11:52:24+08:00","relpermalink":"/zh/publication/wanghr21b/","section":"publication","summary":"Probabilistic programming languages aim to describe and automate Bayesian modeling and inference. Modern languages support programmable inference, which allows users to customize inference algorithms by incorporating guide programs to improve inference performance. For Bayesian inference to be sound, guide programs must be compatible with model programs. One pervasive but challenging condition for model-guide compatibility is absolute continuity, which requires that the model and guide programs define probability distributions with the same support.\n\nThis paper presents a new probabilistic programming language that guarantees absolute continuity, and features general programming constructs, such as branching and recursion. Model and guide programs are implemented as coroutines that communicate with each other to synchronize the set of random variables they sample during their execution. Novel guide types describe and enforce communication protocols between coroutines. If the model and guide are well-typed using the same protocol, then they are guaranteed to enjoy absolute continuity. An efficient algorithm infers guide types from code so that users do not have to specify the types. The new programming language is evaluated with an implementation that includes the type-inference algorithm and a prototype compiler that targets Pyro. Experiments show that our language is capable of expressing a variety of probabilistic models with nontrivial control flow and recursion, and that the coroutine-based computation does not introduce significant overhead in actual Bayesian inference.","tags":[],"title":"Sound Probabilistic Inference via Guide Types","type":"publication"},{"authors":["王迪","Jan Hoffmann","Thomas Reps"],"categories":[],"content":"","date":1617062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1617062400,"objectID":"d76ecb80d763b2564b46315aae241e97","permalink":"https://stonebuddha.github.io/zh/publication/wanghr21c/","publishdate":"2021-09-15T11:57:39+08:00","relpermalink":"/zh/publication/wanghr21c/","section":"publication","summary":"In this article, we present a semantics-level adaption of the Optional Stopping Theorem, sketch an expected-cost analysis as its application, and survey different variants of the Optional Stopping Theorem that have been used in static analysis of probabilistic programs.","tags":[],"title":"Expected-Cost Analysis for Probabilistic Programs and Semantics-Level Adaption of Optional Stopping Theorems","type":"publication"},{"authors":["Ankush Das","王迪","Jan Hoffmann"],"categories":[],"content":"","date":1605657600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1605657600,"objectID":"8deb610d0733a99572c20e9e29f16a89","permalink":"https://stonebuddha.github.io/zh/publication/daswh20/","publishdate":"2021-09-15T12:10:30+08:00","relpermalink":"/zh/publication/daswh20/","section":"publication","summary":"Session types guarantee that message-passing processes adhere to predefined communication protocols. Prior work on session types has focused on deterministic languages but many message-passing systems, such as Markov chains and randomized distributed algorithms, are probabilistic. To model and analyze such systems, this article introduces probabilistic session types and explores their application in automatic expected resource analysis. Probabilistic session types describe probability distributions over messages and are a conservative extension of intuitionistic (binary) session types. To send on a probabilistic channel, processes have to utilize internal randomness from a probabilistic branching expression or external randomness from receiving on a probabilistic channel. The analysis for expected resource bounds is integrated with the type system and is a variant of automatic amortized resource analysis. It can automatically derive symbolic bounds for different cost metrics by reducing type inference to linear constraint solving. The technical contributions include the meta theory that is based on a novel nested multiverse semantics and a type-reconstruction algorithm that allows flexible mixing of different sources of randomness without burdening the programmer with type annotations. The type system has been implemented in the language PRast. Experiments demonstrate that PRast is applicable in different domains such as resource analysis of randomized distributed algorithms, verification of limiting distributions in Markov chains, and analysis of probabilistic digital contracts.","tags":[],"title":"Probabilistic Resource-Aware Session Types","type":"publication"},{"authors":["Tristan Knoth","王迪","Adam Reynolds","Jan Hoffmann","Nadia Polikarpova"],"categories":[],"content":"","date":1598140800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1598140800,"objectID":"891908906372a09b354db8d1cdbd4ad8","permalink":"https://stonebuddha.github.io/zh/publication/knothwr20/","publishdate":"2021-09-15T12:13:40+08:00","relpermalink":"/zh/publication/knothwr20/","section":"publication","summary":"This article presents *liquid resource types*, a technique for automatically verifying the resource consumption of functional programs. Existing resource analysis techniques trade automation for flexibility -- automated techniques are restricted to relatively constrained families of resource bounds, while more expressive proof techniques admitting value-dependent bounds rely on handwritten proofs. Liquid resource types combine the best of these approaches, using logical refinements to automatically prove precise bounds on a program's resource consumption. The type system augments refinement types with potential annotations to conduct an amortized resource analysis. Importantly, users can annotate data structure declarations to indicate how potential is allocated within the type, allowing the system to express bounds with polynomials and exponentials, as well as more precise expressions depending on program values. We prove the soundness of the type system, provide a library of flexible and reusable data structures for conducting resource analysis, and use our prototype implementation to automatically verify resource bounds that previously required a manual proof.","tags":[],"title":"Liquid Resource Types","type":"publication"},{"authors":["王迪","David M. Kahn","Jan Hoffmann"],"categories":[],"content":"","date":1598140800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1598140800,"objectID":"e9891938a37fd75956d111600c8d43d4","permalink":"https://stonebuddha.github.io/zh/publication/wangkh20/","publishdate":"2021-09-15T12:17:33+08:00","relpermalink":"/zh/publication/wangkh20/","section":"publication","summary":"This article presents a type-based analysis for deriving upper bounds on the expected execution cost of probabilistic programs. The analysis is naturally compositional, parametric in the cost model, and supports higher-order functions and inductive data types. The derived bounds are multivariate polynomials that are functions of data structures. Bound inference is enabled by local type rules that reduce type inference to linear constraint solving. The type system is based on the potential method of amortized analysis and extends automatic amortized resource analysis (AARA) for deterministic programs. A main innovation is that bounds can contain symbolic probabilities, which may appear in data structures and function arguments. Another contribution is a novel soundness proof that establishes the correctness of the derived bounds with respect to a distribution-based operational cost semantics that also includes nontrivial diverging behavior. For cost models like time, derived bounds imply termination with probability one. To highlight the novel ideas, the presentation focuses on linear potential and a core language. However, the analysis is implemented as an extension of Resource Aware ML and supports polynomial bounds and user defined data structures. The effectiveness of the technique is evaluated by analyzing the sample complexity of discrete distributions and with a novel average-case estimation for deterministic programs that combines expected cost analysis with statistical methods.","tags":[],"title":"Raising Expectations: Automating Expected Cost Analysis with Types","type":"publication"},{"authors":["Tristan Knoth","王迪","Nadia Polikarpova","Jan Hoffmann"],"categories":[],"content":"","date":1561161600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1561161600,"objectID":"8c9817600be722a6272fb40bfcf9933c","permalink":"https://stonebuddha.github.io/zh/publication/knothwp19/","publishdate":"2021-09-15T12:22:06+08:00","relpermalink":"/zh/publication/knothwp19/","section":"publication","summary":"This article presents resource-guided synthesis, a technique for synthesizing recursive programs that satisfy both a functional specification and a symbolic resource bound. The technique is type-directed and rests upon a novel type system that combines polymorphic refinement types with potential annotations of automatic amortized resource analysis. The type system enables efficient constraint-based type checking and can express precise refinement-based resource bounds. The proof of type soundness shows that synthesized programs are correct by construction. By tightly integrating program exploration and type checking, the synthesizer can leverage the user-provided resource bound to guide the search, eagerly rejecting incomplete programs that consume too many resources. An implementation in the resource-guided synthesizer ReSyn is used to evaluate the technique on a range of recursive data structure manipulations. The experiments show that ReSyn synthesizes programs that are asymptotically more efficient than those generated by a resource-agnostic synthesizer. Moreover, synthesis with ReSyn is faster than a naive combination of synthesis and resource analysis. ReSyn is also able to generate implementations that have a constant resource consumption for fixed input sizes, which can be used to mitigate side-channel attacks.","tags":[],"title":"Resource-Guided Program Synthesis","type":"publication"},{"authors":["王迪","Jan Hoffmann","Thomas Reps"],"categories":[],"content":"","date":1559520000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1559520000,"objectID":"8c71245de524557a2fffb8e48e7fe3ca","permalink":"https://stonebuddha.github.io/zh/publication/wanghr19/","publishdate":"2021-09-15T12:25:42+08:00","relpermalink":"/zh/publication/wanghr19/","section":"publication","summary":"Probabilistic programming is an increasingly popular formalism for modeling randomness and uncertainty. Designing semantic models for probabilistic programs has been extensively studied, but is technically challenging. Particular complications arise when trying to account for (i) unstructured control-flow, a natural feature in low-level imperative programs; (ii) general recursion, an extensively used programming paradigm; and (iii) nondeterminism, which is often used to represent adversarial actions in probabilistic models, and to support refinement-based development. This paper presents a denotational-semantics framework that supports the three features mentioned above, while allowing nondeterminism to be handled in different ways. To support both probabilistic choice and nondeterministic choice, the semantics is given over control-flow *hyper*-graphs. The semantics follows an *algebraic* approach: it can be instantiated in different ways as long as certain algebraic properties hold. In particular, the semantics can be instantiated to support nondeterminism among either *program states* or *state transformers*. We develop a new formalization of nondeterminism based on *powerdomains* over *sub-probability kernels*. Semantic objects in the powerdomain enjoy a notion we call *generalized convexity*, which is a generalization of convexity. As an application, the paper studies the semantic foundations of an algebraic framework for static analysis of probabilistic programs.","tags":[],"title":"A Denotational Semantics for Low-Level Probabilistic Programs with Nondeterminism","type":"publication"},{"authors":["王迪","Jan Hoffmann"],"categories":[],"content":"","date":1547337600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1547337600,"objectID":"696798cbada90d27d9db5c1f6b017964","permalink":"https://stonebuddha.github.io/zh/publication/wangh19/","publishdate":"2021-09-15T12:28:43+08:00","relpermalink":"/zh/publication/wangh19/","section":"publication","summary":"This paper presents a novel technique for type-guided worst-case input generation for functional programs. The technique builds on automatic amortized resource analysis (AARA), a type-based technique for deriving symbolic bounds on the resource usage of functions. Worst-case input generation is performed by an algorithm that takes as input a function, its resource-annotated type derivation in AARA, and a skeleton that describes the shape and size of the input that is to be generated. If successful, the algorithm fills in integers, booleans, and data structures to produce a value of the shape given by the skeleton. The soundness theorem states that the generated value exhibits the highest cost among all arguments of the functions that have the shape of the skeleton. This cost corresponds exactly to the worst-case bound that is established by the type derivation. In this way, a successful completion of the algorithm proves that the bound is tight for inputs of the given shape. Correspondingly, a relative completeness theorem is proved to show that the algorithm succeeds if and only if the derived worst-case bound is tight. The theorem is relative because it depends on a decision procedure for constraint solving. The technical development is presented for a simple first-order language with linear resource bounds. However, the technique scales to and has been implemented for Resource Aware ML, an implementation of AARA for a fragment of OCaml with higher-order functions, user-defined data types, and types for polynomial bounds. Experiments demonstrate that the technique works effectively and can derive worst-case inputs with hundreds of integers for sorting algorithms, operations on search trees, and insertions into hash tables.","tags":[],"title":"Type-Guided Worst-Case Input Generation","type":"publication"},{"authors":["王迪","Jan Hoffmann","Thomas Reps"],"categories":[],"content":"","date":1529452800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1529452800,"objectID":"101d9e3872d9bfa30a1da294e9e55e81","permalink":"https://stonebuddha.github.io/zh/publication/wanghr18/","publishdate":"2021-09-15T12:31:55+08:00","relpermalink":"/zh/publication/wanghr18/","section":"publication","summary":"Automatically establishing that a probabilistic program satisfies some property $\\varphi$ is a challenging problem. While a sampling-based approach---which involves running the program repeatedly---can *suggest* that $\\varphi$ holds, to establish that the program *satisfies* $\\varphi$, analysis techniques must be used. Despite recent successes, probabilistic static analyses are still more difficult to design and implement than their deterministic counterparts. This paper presents a framework, called *PMAF*, for designing, implementing, and proving the correctness of static analyses of probabilistic programs with challenging features such as recursion, unstructured control-flow, divergence, nondeterminism, and continuous distributions. PMAF introduces *pre-Markov algebras* to factor out common parts of different analyses. To perform *interprocedural analysis* and to create *procedure summaries*, PMAF extends ideas from non-probabilistic interprocedural dataflow analysis to the probabilistic setting. One novelty is that PMAF is based on a semantics formulated in terms of a control-flow *hyper-graph* for each procedure, rather than a standard control-flow graph. To evaluate its effectiveness, PMAF has been used to reformulate and implement existing *intraprocedural* analyses for Bayesian-inference and the Markov decision problem, by creating corresponding *interprocedural* analyses. Additionally, PMAF has been used to implement a new interprocedural *linear expectation-invariant analysis*. Experiments with benchmark programs for the three analyses demonstrate that the approach is practical.","tags":[],"title":"PMAF: An Algebraic Framework for Static Analysis of Probabilistic Programs","type":"publication"},{"authors":["Peng Wang","王迪","Adam Chlipala"],"categories":[],"content":"","date":1508889600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1508889600,"objectID":"c6416621277713f368cece0c36217422","permalink":"https://stonebuddha.github.io/zh/publication/wangwc17/","publishdate":"2021-09-15T12:35:14+08:00","relpermalink":"/zh/publication/wangwc17/","section":"publication","summary":"We present TiML (Timed ML), an ML-like functional language with time-complexity annotations in types. It uses indexed types to express sizes of data structures and upper bounds on running time of functions; and refinement kinds to constrain these indices, expressing data-structure invariants and pre/post-conditions. Indexed types are flexible enough that TiML avoids a built-in notion of \"size,\" and the programmer can choose to index user-defined datatypes in any way that helps her analysis. TiML's distinguishing characteristic is supporting highly automated time-bound verification applicable to data structures with nontrivial invariants. The programmer provides type annotations, and the typechecker generates verification conditions that are discharged by an SMT solver. Type and index inference are supported to lower annotation burden, and, furthermore, big-O complexity can be inferred from recurrences generated during typechecking by a recurrence solver based on heuristic pattern matching (e.g. using the Master Theorem to handle divide-and-conquerlike recurrences). We have evaluated TiML's usability by implementing a broad suite of case-study modules, demonstrating that TiML, though lacking full automation and theoretical completeness, is versatile enough to verify worst-case and/or amortized complexities for algorithms and data structures like classic list operations, merge sort, Dijkstra's shortest-path algorithm, red-black trees, Braun trees, functional queues, and dynamic tables with bounds like $mn \\log n$. The learning curve and annotation burden are reasonable, as we argue with empirical results on our case studies. We formalized TiML's type-soundness proof in Coq.","tags":[],"title":"TiML: A Functional Language for Practical Complexity Analysis with Invariants","type":"publication"},{"authors":["唐浩","王迪","熊英飞","Lingming Zhang","Xiaoyin Wang","张路"],"categories":[],"content":"","date":1492819200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1492819200,"objectID":"f10bc7fc2a07bb729f6242653bbc6924","permalink":"https://stonebuddha.github.io/zh/publication/tangwx17/","publishdate":"2021-09-15T12:35:20+08:00","relpermalink":"/zh/publication/tangwx17/","section":"publication","summary":"Library summarization is an effective way to accelerate the analysis of client code. However, information about the client is unknown at the library summarization, preventing complete summarization of the library. An existing approach utilizes tree-adjoining languages (TALs) to provide conditional summaries, enabling the summarization of a library under certain premises. However, the use of TAL imposes several problems, preventing a complete summarization of a library and reducing the efficiency of the analysis. In this paper we propose a new conditional summarization technique based on the context-free language (CFL) reachability analysis. Our technique overcomes the above two limitations of TAL, and is more accessible since CFL reachability is much more efficient and widely-used than TAL reachability. Furthermore, to overcome the high cost from premise combination, we also provide a technique to confine the number of premises while maintaining full summarization of the library. We empirically compared our approach with the state-of-art TAL conditional summarization technique on 12 Java benchmark subjects from the SPECjvm2008 benchmark suite. The results demonstrate that our approach is able to significantly outperform TAL on both efficiency and precision.","tags":[],"title":"Conditional Dyck-CFL Reachability Analysis for Complete and Efficient Library Summarization","type":"publication"},{"authors":["王迪"],"categories":null,"content":" [Nov 2020] 预印本 Probabilistic Resource-Aware Session Types 已上传至 arXiv。 [Jun 2020] 两篇论文 Liquid Resource Types 和 Raising Expectations: Automating Expected Cost Analysis with Types 被 ICFP 2020 接收。 [May 2019] 论文 A Denotational Semantics for Low-Level Probabilistic Programs with Nondeterminism 被 MFPS 接收。 [Feb 2019] Resource-Guided Program Synthesis 被 PLDI 2019 接收。 [Nov 2018] Type-Guided Worst-Case Input Generation 被 POPL 2019 接收。 [Mar 2018] 这是我在 SCS Day 2018 上的表演：Chengdu。 [Feb 2018] PMAF: An Algebraic Framework for Static Analysis of Probabilistic Programs 被 PLDI 2018 接收。 ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":-62135596800,"objectID":"52d880278beeb7d6fedf412fe0ffeb77","permalink":"https://stonebuddha.github.io/zh/news_archive/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/news_archive/","section":"","summary":"[Nov 2020] 预印本 Probabilistic Resource-Aware Session Types 已上传至 arXiv。 [Jun 2020] 两篇论文 Liquid Resource Types 和 Raising","tags":null,"title":"新闻归档","type":"page"}]